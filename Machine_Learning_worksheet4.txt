1. A
2. A
3. A
4. A
5. C
6. C
7. B
8. C
9. Entropy is a way to measure impurity: Entropy=−∑jpjlog2pj
Entropy= -0.6log(0.6)-0.4log(0.4)=0.292
The Gini coefficient is a measure of inequality of a distribution. Gini=1−∑jp2j
Gini Index= 1-(0.6^2+0.4^2) = 0.48
10. One disadvantage of using a single decision tree was that decision trees tend to be prone to overfitting the training data. Random forest is an ensemble method in which a classifier is constructed by combining several different independent base classifiers. The independence is theoretically enforced by training each base classifier on a training set sampled with replacement from the original training set. Further randomness is introduced by identifying the best split feature from a random subset of available features.The ensemble classifier then aggregates the individual predictions to combine into a final prediction, based on a majority voting on the individual predictions.It can be shown that an ensemble of independent classifiers, each with an error rate e, when combined significantly reduces the error rate.

11.Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values. Thus feature Scaling is used to bring all values to same magnitudes.
Techniques to perform Feature Scaling:
i. Min-Max Normalization: This technique re-scales a feature or observation value with distribution value between 0 and 1.
ii. Standardization: It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.

12. Feature scaling helps in causing Gradient Descent to converge much faster as standardizing all the variables on to the same scale. Optimisation techniques allow them to make the best use of data efficiently. One particular trick is maintaining a running mean of gradients over time and adding that to the current gradient.

13. Accuracy is not good metric for unbalanced classification, since it does not distinguish between the numbers of correctly classified examples of different classes.It is usually better to look at the confusion matrix to understand how the classifier is working, or look at metrics other than accuracy such as the precision and recall, F1 score.

14. F1-score, is a measure of a models accuracy on a dataset. It is also defined as the harmonic mean of the models precision and recall.
 f1-score: 2*((precision*recall)/(precision+recall))

15.Fit(): Method calculates the parameters mean(μ) and std(σ) and saves them as internal objects.
Transform(): Method using these same parameters apply the transformation to a particular dataset.
Fit_transform(): joins the fit() and transform() method for transformation of dataset.